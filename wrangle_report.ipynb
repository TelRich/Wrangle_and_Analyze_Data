{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: Wrangle_report\n",
    ">This documentation is about data wrangling steps taken in the project. The aim of the project was to wrangle the WeRateDog [twitter's](https://twitter.com/dog_rates) data and visualize it at the end. The wrangling steps taken to achieve this were, gathering the data, assessing the data and cleaning the data. This report explains in detail in each step of the wrangling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the data\n",
    "> Three datasets were used for the project, each at different locations having different formats from each other. The first data is contained in the `twitter-archive-enhanced.csv` file. The **CSV file** was provided by Udacity to be downloaded manually to the local device.\n",
    "\n",
    "> The second data was contained in the `image-predictions.tsv` file. This **TSV file** was downloaded programmatically using [Request library](https://pypi.org/project/requests/) from the URL https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. It contains the image predictions of each tweet according to a neural network.\n",
    "\n",
    "> The third data was retrieved from twitter using **Twitter's API**. Python's [(Tweepy library)](http://www.tweepy.org/) was used to query the Twitter API for each tweet's **JSON data**. Contained in this JSON data were **each tweet's retweet count** and **favorite(\"like\") count**  needed for the project. I also retrieved the timestamped for cleaning purposes. The entire set of JSON data for each tweet was stored in a file called `tweet_json.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the data\n",
    "> Assessing the data is the second step of data wrangling. In the project this was done in two ways, **visual assessment** where the data was displayed in the Jupyter Notebook for visual assessment purpose and **programmatic assessment** where pandas' functions and/or methods [info(), describe(), sample()] were used. The three dataset were assessed individually and this was done to find quality and tidiness issues.\n",
    "\n",
    "#### Quality issues found in the datasets\n",
    "`twitter-archive-data.csv`\n",
    "\n",
    "* `archv_data` table has retweet rows\n",
    "* unusual name in name column such as \"a\", \"the\" and \"an\"\n",
    "* empty rows of _name_, and _dog_stage_ are filled with _None_ instead of _NaN_\n",
    "* rating_numerator extracted wrongly and it should be float data type.\n",
    "* some rating_denominator are above 10.\n",
    "* erroneous data types (tweet_id and timestamp)\n",
    "* empty dog names in `archv_data` \n",
    "\n",
    "`image-predictions.tsv`\n",
    "\n",
    "* some predictions are not dog breed\n",
    "* tweet id is integer data type\n",
    "* The prediction names contain lowercase, uppercase and underscore. \n",
    "\n",
    "`twtr_data`\n",
    "\n",
    "* erroneous data types (tweet_id and timestamp)\n",
    "\n",
    "#### Tidiness issues found in the dataset\n",
    "* one variable in four columns in `archv_data` table (doggo,floofer,pupper and puppo) \n",
    "* retweet count and favorite count should be part of `archv_data` table\n",
    "* image prediction should be part of `archv_data`\n",
    "\n",
    "> The above was found when assessing the datasets, both visually and programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "> Here, all the listed issues above were cleaned. The data used for cleaning is a copy of the original data. The framework,define-code-test was used to clearly document the process. \n",
    "\n",
    "> Missing values should be handled first, but since there was none in the case of our project, tidiness issues were handled first. During this process, the three datas was merged together and was used for cleaning quality issues. \n",
    "\n",
    "> The end result was a high-quality and tidy master pandas DataFrame  which was saved to the `twitter_archive_master.csv` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
